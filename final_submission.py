# -*- coding: utf-8 -*-
"""final_Submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q4zrYuf-C2S7a0kpMxyU6ecfbgIKCjT3
"""

# step 1 import the numpy library
import numpy as np

# step 2 load the data colleccted from the lander game
file_path = r"final data.csv"

data=[]
with open(file_path, mode="r") as file:
       for line in file:
        row = [float(value.replace('"', '').strip()) for value in line.strip().split(",")]
        data.append(row)
data = np.array(data)
print("Data shape:", data.shape)
print("Data preview:\n", data[:5])

#step 3
# Shuffle the rows randomly
shuffled_indices = np.random.permutation(len(data))
shuffled_data = data[shuffled_indices]

# step 4  preprocessing the data(looking for missing values and duplicated data)
#1- check for missing data
def check_missing_values(shuffled_data):
    missing_count = sum(1 for row in shuffled_data for value in row if value is None or value == '')
    return missing_count
#2-check for duplicated data
def check_duplicates(shuffled_data):
    unique_data = [list(row) for row in set(tuple(row) for row in shuffled_data)]
    duplicate_count = len(shuffled_data) - len(unique_data)
    return duplicate_count
print(f"the amount of missing values are {check_missing_values(data)}")
print(f"the number of duplicated data are {check_duplicates(data)}")

#3- Remove duplicates
cleaned_data = list(set(tuple(row) for row in shuffled_data))
print(f"the length of the cleaned data is {len(cleaned_data)}")

#step 5
# Convert cleaned_data to NumPy array
cleaned_data = np.array(cleaned_data)

# Function to remove outliers based on IQR
def remove_outliers(df):
    # Calculate the first and third quartile (Q1 and Q3) for each feature (column)
    Q1 = np.percentile(df, 25, axis=0)
    Q3 = np.percentile(df, 75, axis=0)

    # Calculate the IQR (Interquartile Range)
    IQR = Q3 - Q1

    # Calculate the lower and upper bounds
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identify outliers (any value outside of the lower and upper bounds)
    outliers = (df < lower_bound) | (df > upper_bound)

    # Remove outliers (retain rows where all features are within the bounds)
    cleaned_data_new = df[~np.any(outliers, axis=1)]

    return cleaned_data_new, outliers

# Remove outliers from the data
cleaned_data_cleaned, outliers_mask = remove_outliers(cleaned_data)

# Print results
print("Shape of cleaned data:", cleaned_data_cleaned.shape)

# step 6 splitting the data to train(70%), test(15%) and validation data(15%)

total_rows = len(cleaned_data_cleaned)
# Calculate split indices
train_end = int(total_rows * 0.7)
test_end = train_end + int(total_rows * 0.15)
# Split the data
train_data = cleaned_data_cleaned[:train_end]
test_data = cleaned_data_cleaned[train_end:test_end]
validation_data = cleaned_data_cleaned[test_end:]

# Split the input and output data for training
train_input = np.array([row[:2] for row in train_data])
train_output = np.array([row[2:] for row in train_data])
# Split the input and output data for testing
test_input = np.array([row[:2] for row in test_data])
test_output = np.array([row[2:] for row in test_data])
# Split the input and output data for Validation
val_input = np.array([row[:2] for row in validation_data])
val_output = np.array([row[2:] for row in validation_data])

# Step 7: Calculate min and max from training data
train_input_min = train_input.min(axis=0)
train_input_max = train_input.max(axis=0)
train_output_min = train_output.min(axis=0)
train_output_max = train_output.max(axis=0)
print(f"input minimum are{train_input_min} and input maximum are{train_input_max}")
print(f"output minimum are{train_output_min} and output maximum are{train_output_max}")

#step 8 save min and max value of the trainning data
np.savez('train data min max value.npz',
                 train_input_min = train_input_min,
                 train_input_max =train_input_max,
                 train_output_min = train_output_min,
                 train_output_max = train_output_max)

# step 9 Normalizing the data using min max normalization method
# train inputs
normalized_train_inputs = []

for row in train_input:
    normalized_train_inputs_row = []
    for i in range(len(row)):
        if train_input_max[i] - train_input_min[i] == 0:
            normalized_value = 0
        else:
            normalized_value = (row[i] - train_input_min[i]) / (train_input_max[i] - train_input_min[i])
        normalized_train_inputs_row.append(normalized_value)
    normalized_train_inputs.append(normalized_train_inputs_row)

# Convert the result back to a NumPy array
normalized_train_inputs = np.array(normalized_train_inputs)

# Print shape and preview
print("Shape of normalized training inputs:", normalized_train_inputs.shape)
print("First 5 rows of normalized training inputs:\n", normalized_train_inputs[:5])

# train outputs
normalized_train_outputs = []

for row in train_output:
    normalized_train_outputs_row = []
    for i in range(len(row)):
        if train_output_max[i] - train_output_min[i] == 0:
            normalized_value = 0
        else:
            normalized_value = (row[i] - train_output_min[i]) / (train_output_max[i] - train_output_min[i])
        normalized_train_outputs_row.append(normalized_value)
    normalized_train_outputs.append(normalized_train_outputs_row)

# Convert the result back to a NumPy array
normalized_train_outputs = np.array(normalized_train_outputs)

# Print shape and preview
print("Shape of normalized training outputs:", normalized_train_outputs.shape)
print("First 5 rows of normalized training outputs:\n", normalized_train_outputs[:5])

# Normalizing the validation data
# validation inputs
normalized_val_inputs = []

for row in val_input:
    normalized_val_inputs_row = []
    for i in range(len(row)):
        if train_input_max[i] - train_input_min[i] == 0:
            normalized_value = 0
        else:
            normalized_value = (row[i] - train_input_min[i]) / (train_input_max[i] - train_input_min[i])
        normalized_val_inputs_row.append(normalized_value)
    normalized_val_inputs.append(normalized_val_inputs_row)

# Convert the result back to a NumPy array
normalized_val_inputs = np.array(normalized_val_inputs)

# Print shape and preview
print("Shape of normalized validation inputs:", normalized_val_inputs.shape)
print("First 5 rows of normalized val inputs:\n", normalized_val_inputs[:5])

# validation outputs
normalized_val_outputs = []

for row in val_output:
    normalized_val_outputs_row = []
    for i in range(len(row)):
        if train_output_max[i] - train_output_min[i] == 0:
            normalized_value = 0
        else:
            normalized_value = (row[i] - train_output_min[i]) / (train_output_max[i] - train_output_min[i])
        normalized_val_outputs_row.append(normalized_value)
    normalized_val_outputs.append(normalized_val_outputs_row)

# Convert the result back to a NumPy array
normalized_val_outputs = np.array(normalized_val_outputs)

# Print shape and preview
print("Shape of normalized validation outputs:", normalized_val_outputs.shape)
print("First 5 rows of normalized val outputs:\n", normalized_val_outputs[:5])

# Normalizing the test data
# test inputs
normalized_test_inputs = []

for row in test_input:
    normalized_test_inputs_row = []
    for i in range(len(row)):
        if train_input_max[i] - train_input_min[i] == 0:
            normalized_value = 0
        else:
            normalized_value = (row[i] - train_input_min[i]) / (train_input_max[i] - train_input_min[i])
        normalized_test_inputs_row.append(normalized_value)
    normalized_test_inputs.append(normalized_test_inputs_row)

# Convert the result back to a NumPy array
normalized_test_inputs = np.array(normalized_test_inputs)

# Print shape and preview
print("Shape of normalized test inputs:", normalized_test_inputs.shape)
print("First 5 rows of normalized test inputs:\n", normalized_test_inputs[:5])

# test outputs
normalized_test_outputs = []

for row in test_output:
    normalized_test_outputs_row = []
    for i in range(len(row)):
        if train_output_max[i] - train_output_min[i] == 0:
            normalized_value = 0
        else:
            normalized_value = (row[i] - train_output_min[i]) / (train_output_max[i] - train_output_min[i])
        normalized_test_outputs_row.append(normalized_value)
    normalized_test_outputs.append(normalized_test_outputs_row)

# Convert the result back to a NumPy array
normalized_test_outputs = np.array(normalized_test_outputs)

# Print shape and preview
print("Shape of normalized test outputs:", normalized_test_outputs.shape)
print("First 5 rows of normalized test outputs:\n", normalized_test_outputs[:5])

#step 10: combine the input and output of the data
# Combine the normalized train inputs and outputs
combined_train_data = np.hstack((normalized_train_inputs, normalized_train_outputs))
# Combine the normalized validation inputs and outputs
combined_val_data = np.hstack((normalized_val_inputs, normalized_val_outputs))
# Combine the normalized test inputs and outputs
combined_test_data = np.hstack((normalized_test_inputs, normalized_test_outputs))

#step 11 save the scaled data
# Save normalized trainning data to CSV
with open(r"C:\Users\DataScience\Documents\DS\Assignments\Autumn\Neural Network\ass 1 neural network\final\normalized_train_data.csv", mode='w') as file:
    for row in combined_train_data :
        file.write(",".join(map(str, row)) + "\n")
# Save normalized validation data to CSV
with open(r"C:\Users\DataScience\Documents\DS\Assignments\Autumn\Neural Network\ass 1 neural network\final\normalized_val_data.csv", mode='w') as file:
    for row in combined_val_data :
        file.write(",".join(map(str, row)) + "\n")
# Save normalized test data to CSV
with open(r"C:\Users\DataScience\Documents\DS\Assignments\Autumn\Neural Network\ass 1 neural network\final\normalized_test_data.csv", mode='w') as file:
    for row in combined_test_data :
        file.write(",".join(map(str, row)) + "\n")

#Step 12: Define the Neuron class to implement the feedforward and backpropagation process
import numpy as np

class Neuron:
    def __init__(self, input_size, hidden_size, output_size, learning_rate, momentum):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        self.momentum = momentum

        # Initialize weights and biases
        self.weight = np.random.rand(input_size, hidden_size)
        self.weight2 = np.random.rand(hidden_size, output_size)
        self.bias_hidden = np.random.rand(hidden_size)
        self.bias_output = np.random.rand(output_size)

        # Initialize velocity terms for momentum
        self.velocity_weight = np.zeros_like(self.weight)
        self.velocity_weight2 = np.zeros_like(self.weight2)
        self.velocity_bias_hidden = np.zeros_like(self.bias_hidden)
        self.velocity_bias_output = np.zeros_like(self.bias_output)

    def activation(self, v):
        return 1 / (1 + np.exp(-0.9*v))  # Sigmoid activation

    def sigmoid_derivative(self, v):
        return 0.9*v * (1 - v)


    def forward(self, input_data):
        # Calculate hidden layer
        self.hidden_input = np.dot(input_data, self.weight) + self.bias_hidden
        self.hidden_output = self.activation(self.hidden_input)

        # Calculate output layer
        self.output_input = np.dot(self.hidden_output, self.weight2) + self.bias_output
        self.final_output = self.output_input  # Linear activation

        return self.final_output

    def backward(self, input_data, output_data):
        # Output error and gradient
        output_error = output_data - self.final_output
        output_gradient = output_error  # Linear activation, so derivative is 1

        # Update the weights in the output layer
        weight2_update = self.learning_rate * np.dot(self.hidden_output.reshape(-1, 1), output_gradient.reshape(1, -1))
        self.velocity_weight2 = self.momentum * self.velocity_weight2 + weight2_update
        self.weight2 += self.velocity_weight2

        # Update bias weight for output layer
        bias_output_update = self.learning_rate * output_gradient
        self.velocity_bias_output = self.momentum * self.velocity_bias_output + bias_output_update
        self.bias_output += self.velocity_bias_output

        # Calculate the hidden error
        hidden_error = np.dot(self.weight2, output_gradient.reshape(-1, 1)).T

        # Calculate the hidden gradient
        hidden_gradient = self.sigmoid_derivative(self.hidden_output) * hidden_error.reshape(-1)

        # Update the weights in the hidden layer
        weight_update = self.learning_rate * np.dot(input_data.reshape(-1, 1), hidden_gradient.reshape(1, -1))
        self.velocity_weight = self.momentum * self.velocity_weight + weight_update
        self.weight += self.velocity_weight

        # Update bias weight for hidden layer
        bias_hidden_update = self.learning_rate * hidden_gradient
        self.velocity_bias_hidden = self.momentum * self.velocity_bias_hidden + bias_hidden_update
        self.bias_hidden += self.velocity_bias_hidden


# get the weights (input-to-hidden and hidden-to-output)
    def get_weights(self):
        return self.weight, self.weight2

# get the biases weight (hidden and output)
    def get_biases(self):
        return self.bias_hidden, self.bias_output
# set the weights (input-to-hidden and hidden-to-output)
    def set_weights(self, new_weights):
        self.weight, self.weight2 = new_weights

# set the biases weight (hidden and output)
    def set_biases(self, new_biases):
        self.bias_hidden, self.bias_output = new_biases

#Step 13 initialize the neural network with hyperparameters and train the model for 100 epoch
# Initialize the neural network
network = Neuron(input_size=2, hidden_size=9, output_size=2, learning_rate=0.0001, momentum=0.8)

# Define the number of epochs and early stopping parameters
epochs = 100
patience = 10
training_losses = []
validation_losses = []

# Initialize variables to track the best validation loss and corresponding weights and biases
best_val_loss = float('inf')
best_weights = None
best_biases = None
no_improvement_count = 0

# Training loop with the first epoch in sequence, the rest with shuffling
for epoch in range(epochs):
    epoch_train_loss = 0
    if epoch == 0:
        # First epoch: process data in sequence
        for i in range(len(normalized_train_inputs)):
            # Forward pass on training data
            train_prediction = network.forward(normalized_train_inputs[i])

            # Compute training loss (Root Mean Squared Error)
            train_loss = np.sqrt(np.mean((train_prediction - normalized_train_outputs[i]) ** 2))
            epoch_train_loss += train_loss

            # Backward pass to update weights
            network.backward(normalized_train_inputs[i], normalized_train_outputs[i])

    else:
        # Subsequent epochs: shuffle training data
        indices = np.random.permutation(len(normalized_train_inputs))
        for i in indices:
            # Forward pass on training data
            train_prediction = network.forward(normalized_train_inputs[i])

            # Compute training loss (Root Mean Squared Error)
            train_loss = np.sqrt(np.mean((train_prediction - normalized_train_outputs[i]) ** 2))
            epoch_train_loss += train_loss

            # Backward pass to update weights
            network.backward(normalized_train_inputs[i], normalized_train_outputs[i])

    # Average training loss for the epoch
    epoch_train_loss /= len(normalized_train_inputs)
    training_losses.append(epoch_train_loss)
    print(f"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_train_loss:.4f}")

    # Validation loop (without backpropagation)
    epoch_val_loss = 0
    for j in range(len(normalized_val_inputs)):
        # Forward pass on validation data
        val_prediction = network.forward(normalized_val_inputs[j])

        # Compute validation loss (Root Mean Squared Error)
        val_loss = np.sqrt(np.mean((val_prediction - normalized_val_outputs[j]) ** 2))
        epoch_val_loss += val_loss

    # Average validation loss for the epoch
    epoch_val_loss /= len(normalized_val_inputs)
    validation_losses.append(epoch_val_loss)
    print(f"Epoch {epoch+1}/{epochs}, Validation Loss: {epoch_val_loss:.4f}")

    # Check if this epoch has the best validation loss
    if epoch_val_loss < best_val_loss:
        best_val_loss = epoch_val_loss
        best_weights = network.get_weights()
        best_biases = network.get_biases()
        no_improvement_count = 0
        print(f"New best validation loss: {best_val_loss:.4f} - Saving weights and biases")

     # Save the best weights and biases to files
        np.savez('best_weights_and_biases.npz',
                 weight =best_weights[0],
                 weight2 =best_weights[1],
                 bias_hidden = best_biases[0],
                 bias_output = best_biases[1])

        print(f"Best weights and biases saved at epoch {epoch+1}")
    else:
        no_improvement_count += 1

    # Early stopping condition
    if no_improvement_count >= patience:
        print(f"Early stopping triggered at epoch {epoch+1}")
        break
        print("Training complete.")

# step 14 visualize the error vs epoch for trainning and validation data
import matplotlib.pyplot as plt

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(training_losses, label="Training Loss", color="blue")
plt.plot(validation_losses, label="Validation Loss", color="orange")

# Adding titles and labels
plt.title("Training and Validation Loss Over Epochs", fontsize=16)
plt.xlabel("Epochs", fontsize=14)
plt.ylabel("Loss", fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)
plt.show()

#step 15 printing the value of weights saved for best performance
print("Updated hidden weights:", network.weight)
print("Updated output weights:", network.weight2)
print("Updated hidden biases:", network.bias_hidden)
print("Updated output biases:", network.bias_output)

#step 16 finally predict and calculate the root mean squared error on test(unseen)data and evaluate the model

# Load the best weights and biases
best_weights = np.load('best_weights_and_biases.npz')


network.set_weights([best_weights['weight'], best_weights['weight2']])
network.set_biases([best_weights['bias_hidden'], best_weights['bias_output']])

# Make predictions using the test input
predictions = []
for input_data in normalized_test_inputs:
    output = network.forward(input_data)
    predictions.append(output)
predictions = np.array(predictions)

# Denormalize the predictions
denormalized_pred = predictions * (train_output_max - train_output_min) + train_output_min

# Denormalize the test outputs
denormalized_test_output = normalized_test_outputs * (train_output_max - train_output_min) + train_output_min

def Root_mean_squared_error(y_true,y_pred):
    return np.sqrt(np.mean((y_true-y_pred)**2))

# calculating the root mean square errorfor the test data
RMSE = Root_mean_squared_error(normalized_test_outputs,predictions)
print(f"the root mean squared error is",{RMSE})

